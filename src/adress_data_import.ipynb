{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf3\n",
    "# !pip install tabula-py\n",
    "# !pip install camelot-py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import camelot as camel\n",
    "from PyPDF3 import PdfFileReader \n",
    "from tabula import read_pdf \n",
    "from tabulate import tabulate\n",
    "\n",
    "from my_modules import my_functions as mybib\n",
    "from my_modules import project_functions as pr\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# https://www.youtube.com/watch?v=702lkQbZx50 - for tabula\n",
    "# https://stackoverflow.com/questions/45457054/tabula-extract-tables-by-area-coordinates - for tabula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: close pdf file when error occurs\n",
    "\n",
    "def extract_all_pages_from_pdf(path, file_name):\n",
    "    # open path as binary / rb = read binary -> used for pdf files (https://www.youtube.com/watch?v=OZo2HxoIOtw)\n",
    "            \n",
    "    pdf = PdfFileReader(open(path, 'rb'))\n",
    "    last_page = pdf.getNumPages()\n",
    "    last_page += 1\n",
    "\n",
    "    # defining page range for import \n",
    "    [i for i in range(0, 15)]\n",
    "    page_range = [i for i in range(5, last_page)]\n",
    "    print(page_range)\n",
    "\n",
    "    # prepare column name filler\n",
    "    column_names = [i for i in range(0, 14)]\n",
    "    print(column_names)\n",
    "\n",
    "    # create empty dataframe\n",
    "    adress_data = pd.DataFrame(columns = column_names)\n",
    "    display(adress_data)\n",
    "    \n",
    "    # extract district information \n",
    "    #TODO: nochmal pr√ºfen und vielleicht optimieren\n",
    "    district = [re.findall(r'(?<=adr_)(.*?)(?=_\\d{4}\\.pdf)', file_name)]\n",
    "    print(district)\n",
    "\n",
    "    for page in page_range:\n",
    "        \n",
    "        import_data = read_pdf(path, pages = page, encoding = 'ISO-8859-1', stream = True, area = [175, 33, 783, 520], guess = True, pandas_options={'header': None})\n",
    "        table_df = import_data[0]\n",
    "\n",
    "        \n",
    "        table_df = import_data[0]\n",
    "        columns_len = len(table_df.columns)\n",
    "\n",
    "        if columns_len < 14:\n",
    "            if table_df.iloc[:, 3].dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "                table_df.insert(3, 'm1', np.nan)\n",
    "\n",
    "            if table_df.iloc[:, 6].dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "                table_df.insert(5, 'm2', np.nan)\n",
    "            \n",
    "            \n",
    "            column_length = len(table_df.columns)\n",
    "\n",
    "            \n",
    "            if column_length < 14:\n",
    "                table_df['m3'] = np.nan\n",
    "                # display(table_df)\n",
    "            \n",
    "            \n",
    "            table_df.columns = column_names\n",
    "            table_df['14'] = str(district[0][0])\n",
    "            \n",
    "            adress_data = pd.concat([adress_data, table_df], ignore_index=True , axis=0)\n",
    "            \n",
    "            print(f'page: {page}')\n",
    "            print(adress_data.shape)\n",
    "    \n",
    "    return adress_data, district[0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_adress_data_by_year(year):\n",
    "    \n",
    "    \n",
    "    list_of_files = os.listdir(f'../data/input/berlin_adresses/{year}')\n",
    "    list_of_files\n",
    "        \n",
    "    \n",
    "    for file_name in list_of_files:\n",
    "        path = f'../data/input/berlin_adresses/{year}/{file_name}'\n",
    "        print(path)\n",
    "        \n",
    "        dataset, name = extract_all_pages_from_pdf(path, file_name)\n",
    "        \n",
    "        dataset.to_csv(f'../data/output/temp_adress_data/{name}-{year}.csv', index = False)\n",
    "        display(dataset.head(5))\n",
    "        print(dataset.shape)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_adress_data_by_year(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_adress_data_by_year(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'adr_friedrichshain-kreuzberg_2020.pdf'\n",
    "\n",
    "# district = [re.findall(r'(?<=adr_)(.*?)(?=_\\d{4}\\.pdf)', file_name)]\n",
    "# district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../data/input/berlin_adresses/2023/adr_friedrichshain-kreuzberg_2023.pdf'\n",
    "\n",
    "# extract_all_pages_from_pdf(path, 'adr_friedrichshain-kreuzberg_2023.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
